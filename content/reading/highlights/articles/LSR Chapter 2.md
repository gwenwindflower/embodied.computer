# LSR Chapter 2

![rw-book-cover](http://learningstatisticswithr.com/images/jasmine-faint.jpg)

## Metadata
- Author: [[Danielle Navarro (bookdown translation: Emily Kothe)]]
- Full Title: LSR Chapter 2
- Category: #articles
- Document Tags: [[data]] [[statistics]] 
- URL: https://learningstatisticswithr.com/book/studydesign.html#some-complexities

## Highlights
- Now, when we doing an analysis, we have different names for XX and YY, since they play different roles in the analysis. The classical names for these roles are ***independent variable*** (IV) and ***dependent variable*** (DV). The IV is the variable that you use to do the explaining (i.e., XX) and the DV is the variable being explained (i.e., YY). The logic behind these names goes like this: if there really is a relationship between XX and YY then we can say that YY depends on XX, and if we have designed our study “properly” then XX isn’t dependent on anything else. However, I personally find those names horrible: they’re hard to remember and they’re highly misleading, because (a) the IV is never actually “independent of everything else” and (b) if there’s no relationship, then the DV doesn’t actually depend on the IV. And in fact, because I’m not the only person who thinks that IV and DV are just awful names, there are a number of alternatives that I find more appealing. The terms that I’ll use in these notes are ***predictors*** and ***outcomes***. The idea here is that what you’re trying to do is use XX (the predictors) to make guesses about YY (the outcomes).[9](https://learningstatisticswithr.com/book/studydesign.html#fn9) This is summarised in Table [2.2](https://learningstatisticswithr.com/book/studydesign.html#tab:ivdv). ([View Highlight](https://read.readwise.io/read/01gmksfzt0gaq256s4qd6m5q56))
- ***Internal validity*** refers to the extent to which you are able draw the correct conclusions about the causal relationships between variables. ([View Highlight](https://read.readwise.io/read/01gmkw2cv2vancdtwrzt98839p))
- ***External validity*** relates to the ***generalisability*** of your findings. That is, to what extent do you expect to see the same pattern of results in “real life” as you saw in your study. ([View Highlight](https://read.readwise.io/read/01gmkw2nscm2g7gv926wrst2s0))
- ***Construct validity*** is basically a question of whether you’re measuring what you want to be measuring. A measurement has good construct validity if it is actually measuring the correct theoretical construct, and bad construct validity if it doesn’t. ([View Highlight](https://read.readwise.io/read/01gmkw3h7jv0wmynxttcaqqwnn))
- ***Face validity*** simply refers to whether or not a measure “looks like” it’s doing what it’s supposed to, nothing more. If I design a test of intelligence, and people look at it and they say “no, that test doesn’t measure intelligence”, then the measure lacks face validity. ([View Highlight](https://read.readwise.io/read/01gmkw4zsap387mga8j50yhtjk))
- ***Confound***: A confound is an additional, often unmeasured variable that turns out to be related to both the predictors and the outcomes. The existence of confounds threatens the internal validity of the study because you can’t tell whether the predictor causes the outcome, or if the confounding variable causes it, etc. ([View Highlight](https://read.readwise.io/read/01gmkx00pk2w4cdvat3vxem8vr))
- ***Artifact***: A result is said to be “artifactual” if it only holds in the special situation that you happened to test in your study. The possibility that your result is an artifact describes a threat to your external validity, because it raises the possibility that you can’t generalise your results to the actual population that you care about. ([View Highlight](https://read.readwise.io/read/01gmkx020ksdzr0hd96s9xb56m))
- ***History effects*** refer to the possibility that specific events may occur during the study itself that might influence the outcomes. For instance, something might happen in between a pre-test and a post-test. ([View Highlight](https://read.readwise.io/read/01gmkx90d8fke09tmz2h2ea4fr))
- As with history effects, ***maturational effects*** are fundamentally about change over time. However, maturation effects aren’t in response to specific events. Rather, they relate to how people change on their own over time: we get older, we get tired, we get bored, etc. ([View Highlight](https://read.readwise.io/read/01gmkxgc6wrfhe5yajnm3w94nj))
- An important type of history effect is the effect of ***repeated testing***. Suppose I want to take two measurements of some psychological construct (e.g., anxiety). One thing I might be worried about is if the first measurement has an effect on the second measurement. In other words, this is a history effect in which the “event” that influences the second measurement is the first measurement itself! ([View Highlight](https://read.readwise.io/read/01gmkxhtnc5y15mfs6fmcp55am))
- The first is ***homogeneous attrition***, in which the attrition effect is the same for all groups, treatments or conditions. In the example I gave above, the differential attrition would be homogeneous if (and only if) the easily bored participants are dropping out of all of the conditions in my experiment at about the same rate. In general, the main effect of homogeneous attrition is likely to be that it makes your sample unrepresentative. As such, the biggest worry that you’ll have is that the generalisability of the results decreases: in other words, you lose external validity. ([View Highlight](https://read.readwise.io/read/01gmkyrsdfmhw0s2ehy9anpbks))
- The second type of differential attrition is ***heterogeneous attrition***, in which the attrition effect is different for different groups. This is a much bigger problem: not only do you have to worry about your external validity, you also have to worry about your internal validity too. ([View Highlight](https://read.readwise.io/read/01gmkys20rv132k08g4z786xd8))
- ***Non-response bias*** is closely related to selection bias, and to differential attrition. The simplest version of the problem goes like this. You mail out a survey to 1000 people, and only 300 of them reply. The 300 people who replied are almost certainly not a random subsample. People who respond to surveys are systematically different to people who don’t. ([View Highlight](https://read.readwise.io/read/01gmkysytrjtm0xm7j1qx4egq2))
- However, if the question that 80 people didn’t answer was the most confrontational or invasive personal question in the questionnaire, then almost certainly you’ve got a problem. In essence, what you’re dealing with here is what’s called the problem of ***missing data***. If the data that is missing was “lost” randomly, then it’s not a big problem. If it’s missing systematically, then it can be a big problem. ([View Highlight](https://read.readwise.io/read/01gmkyv0dfyazsv6zxkn293e33))
- ***Regression to the mean*** is a curious variation on selection bias. It refers to any situation where you select data based on an extreme value on some measure. Because the measure has natural variation, it almost certainly means that when you take a subsequent measurement, that later measurement will be less extreme than the first one, purely by chance. ([View Highlight](https://read.readwise.io/read/01gmkyxe32ngha6fc07dp05z25))
- And that’s the thing: intelligence and hard work are transferrable from one class to the next. Luck isn’t. The people who got lucky in high school won’t be the same as the people who get lucky at university. That’s the very definition of “luck”. The consequence of this is that, when you select people at the very extreme values of one measurement (the top 20 students), you’re selecting for hard work, skill and luck. But because the luck doesn’t transfer to the second measurement (only the skill and work), these people will all be expected to drop a little bit when you measure them a second time (at university). So their scores fall back a little bit, back towards everyone else. This is regression to the mean. ([View Highlight](https://read.readwise.io/read/01gmkyzqjwy2cjp7bhhnjh7nct))
- ***Experimenter bias*** can come in multiple forms. The basic idea is that the experimenter, despite the best of intentions, can accidentally end up influencing the results of the experiment by subtly communicating the “right answer” or the “desired behaviour” to the participants. Typically, this occurs because the experimenter has special knowledge that the participant does not – either the right answer to the questions being asked, or knowledge of the expected pattern of performance for the condition that the participant is in, and so on. ([View Highlight](https://read.readwise.io/read/01gmkz2kskntqdfq5jspj18j3b))
- The general solution to the problem of experimenter bias is to engage in double blind studies, where neither the experimenter nor the participant knows which condition the participant is in, or knows what the desired behaviour is. ([View Highlight](https://read.readwise.io/read/01gmkz1z4k56axfysaz6ebw45e))
- it’s almost impossible to stop people from knowing that they’re part of a psychological study. And the mere fact of knowing that someone is watching/studying you can have a pretty big effect on behaviour. This is generally referred to as ***reactivity*** or ***demand effects***. The basic idea is captured by the Hawthorne effect: people alter their performance because of the attention that the study focuses on them. ([View Highlight](https://read.readwise.io/read/01gmkz4r2sqsbjj8s38c0qhtpq))
- The ***placebo effect*** is a specific type of demand effect that we worry a lot about. It refers to the situation where the mere fact of being treated causes an improvement in outcomes. The classic example comes from clinical trials: if you give people a completely chemically inert drug and tell them that it’s a cure for a disease, they will tend to get better faster than people who aren’t treated at all. In other words, it is people’s belief that they are being treated that causes the improved outcomes, not the drug. ([View Highlight](https://read.readwise.io/read/01gmkz60b68k4akdwmyfzs2xgk))
- *It is difficult to get a man to understand something, when his salary depends on his not understanding it.*
  – Upton Sinclair ([View Highlight](https://read.readwise.io/read/01gmkzdtzk5b04tmqzd4jvdax9))
- ***Data misrepresentation***. While fraud gets most of the headlines, it’s much more common in my experience to see data being misrepresented. When I say this, I’m not referring to newspapers getting it wrong (which they do, almost always). I’m referring to the fact that often, the data don’t actually say what the researchers think they say. My guess is that, almost always, this isn’t the result of deliberate dishonesty, it’s due to a lack of sophistication in the data analyses. For instance, think back to the example of Simpson’s paradox that I discussed in the beginning of these notes. It’s very common to see people present “aggregated” data of some kind; and sometimes, when you dig deeper and find the raw data yourself, you find that the aggregated data tell a different story to the disaggregated data. Alternatively, you might find that some aspect of the data is being hidden, because it tells an inconvenient story (e.g., the researcher might choose not to refer to a particular variable). There’s a lot of variants on this; many of which are very hard to detect. ([View Highlight](https://read.readwise.io/read/01gmkzfs3tkh7e9wf1va0wb9gt))
- if you keep trying to analyse your data in lots of different ways, you’ll eventually find something that “looks” like a real effect but isn’t. This is referred to as “data mining”. It used to be quite rare because data analysis used to take weeks, but now that everyone has very powerful statistical software on their computers, it’s becoming very common. ([View Highlight](https://read.readwise.io/read/01gmkzgqg796fb8snhbf0hnd9b))
